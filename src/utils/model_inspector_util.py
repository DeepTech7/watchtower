# (((((((((((((##########%%%#((((((((((((((((((((((((((((((((((((((((##%%%%%%%%%%###%#/,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
# (((((((((((((((((##########%%%##(((((((((((((((((((((((((((((##%%%%%%%%%%####(*,........,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
# (((((((((((((((((((((((#########%%%%&&&&%#(((((((((##%&&&&&&%%%%%%##%##/,,,..,...,,,.......,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
# ((((((((((((((((((((((((((((#&@#((/((((((((((((((((((((((((((((#@%...,,,,...................,,,,,,,,,,,,,,,,,,,,,,,,,,,,
# (((((((((((((((((((((((((%@(((((((((((((((((((((((((((((((((((((((((#&&*...................,..,,,,,,,,,,,,,,,,,,,,,,,,,,
# ((((((((((((((((((((#((&#((((((((((((((((((((((((((((((((((((((((((((((((%&/..,.,...............,,,,,,,,,,,,,,,,,,,,,,,,
# %%%#(((((((((((((((((&(((((((((((((((((((((((((((((((((((((((((((((((((((((((&#,...,.............,,,,,,,,,,,,,,,,,,,,,,,
# ####%%###((((((((((%%((((((((((((((((((((((((((/((((((((((((((((((((((((((((((((%#,..,,........,...,,,,,,,,,,,,,,,,,,,,,
# ((#######%%%##(((#&#((((((((((((((((((((((((/((//((((((((((((((((((((((((((((((((((&/................,,,,,,,,,,,,,,,,,,,
# ((((((#####%%%%%%&#((((((((((((((((((((((##//*///(%((((((((((((((((((((((((((((((((((%%////*.....,....,,,,,,,,,,,,,,,,,,
# ((((##%%%%%%%%%%&%((((((((((((((((((((((#///%(*////*#((((((((#(((((((((((((((((((((((((#%(((((%&*.,..,..,,,,,,,,,,,,,,,,
# %%%%%%##%#(/,.,.%((((((((((((((((((((((#*/((#&(//*/*/%(((((((/(%#(((((((((((((((((((((((((%(((((%(.,......,,,,,,,,,,,,,,
# %%##/....,,....,%((((((((((((((((((((((%**/AISHIELD*/#((((((((((/(%#(((((((((((((((((((((((((((((&.,.......,,,,,,,,,,,,,
# ................/&((((((((((&(/(((((((((#(*/(((/(**(#(((((((((((((((((%#((((((((((((((((((((((((&*.,,........,,,,,,,,,,,
# ................./#(((((((((/%((((((((((/(((#%%%##(((((((((((((((((((((((((%%#/(((((((((((((((&(..,..........,,,,,,,,,,,
# ................,.*@((((((((((%(((((((((((((((((((((((((#######%%#####((((((((((((###%%#&%(*,.....,.........,...,,,,,,,,
# ..................,.,&%((((((((#(((((((/((#%&&&&&&&%%%&&&&&&&&&&&&%%%&%&%&%%%%%%%&&&&&&%%@,..,.................,..,,,,,,
# .................,,....,,/#&&%#%%((#%&&%&%&&%%%%%%%%%%%%%%%%%%%%%%&&&&&%%%%%%%%%&&&&&%&&%&(,*,,..................,..,,,,
# .............................,,.(&%%%%%&%&%%%&&%%%%%%&&&&@&&%####(%#/%@@@%///////&@&&@@@&(**///&,,.,.................,,,
# ................................,#&%&%%&%%&&&%##((((((((((((((%&&@@&(////*/////(###&@&&%///#(///%/.....................,
# ................................,.&&%#((((((((((((((((((##&@@@%#/*/((#%(/*,&@@@(//*/#&&//#///////&,,...................,
# ................................,%((((((((((((((#%%(/////#%@@@@@@@@@@&,,.,@@@@@&*///*##//&(((////&,,................,,..
# ........,..,...,............,..(%((##&%#(/*/(%%&@@@@//////&@@@@@@@@@%.,,,@@@@@@@*/////*/////#///(#,.....................
# ......,...*@/*##...........,.,.../&@@@@@@@#.,.(@@@@@%#/////%@@@@@@@(,,.*@@@@@@@%///////**//////&*..,....................
# ...........#(///%(..,.............&@@@@@@*,..#@@@@@@//////*//%@@@@*,,.,@@@@@@@@///////#&%%#%%(,.........................
# .......,.,,,,&(/*/&*.,..........,,,&@@@&,.,.(@@@@@@(////////////%/,.,/@@@@@@%/*///////#*.....,,.........................
# ......,....,..*&///(%..,............#@%,.,,#@@@@@@///*(#/*///////////*//////////*////&*......,,.........................
# .......,.,.,#(/*#//*/&,,,.........,...(%,.#@@@@&(//*//*///////////////////////////*#%,......,.,.........................
# .......,,##/((//#/*///%/...............,&///**///////////(@&&&#///////////////*//##/////#((%&(,,........................
# .......,%##/#%((##%(((/((.,..........,.,,&(*///*//////////&&@%&//////////////*/%((((//((%/(((/((%%/,..,.................
# ......,(%//#(*//////////(%....,...........,#&(/////////////(#///////////*/*(%(((((((//(#((((((((((%((&#,.,,.............
# ........,&%/%/*/#/*///*//((,..............,.,,*&&(///////////*/*///////#&#(/((((((((/*(##(((((((##((%///*%%,.,..........
# ..........,&(/////#/*//////&,.,........,,...#&#((/((((#&#(//*//**(&#((/(/#((((((((/((*/(%((((//(#((#/////////&/...,,....
# ............,(%/*////*///////%*........./&#((((%(((((((%((((((##*#(((((##&((((((((((((*(#%(((((&((#///////*/*///#%,.,...
# ...........,,..,&/////////////*%(...,(&###((((((%((((((%((((##%#(%((@&((((//////((%%(((*/(%*%&%((((/*//////////////#%,..
# ................,,%*/////////*//*#@%//(#((##(((((%/(((((((/(((((((%/(((((/,*/((#%&%#%(((//%/,,.,#&(///////////////////&/
# .................,.*%////////*//*///%//*/#((#(((((%((((((((#%@%(,%(%(((((%((((((((%#(#((//(#.,,.,,,./&(/////////////////
# ................,.,..*&//////////////////*(#((%(&/,@(((&(%###&%#(#(#((((((#/*/(##((/(&(*/((&.,........,./&////*/////////
# .....................,.*&///////////**///*//%&/,,..,(%((#/(((((##(##%(((((#((/*//((/*/#((/(&,........,,.,..##*//////////
# ......................,.,,&(/////////////(&*...,.....,&((%(((((((((#(##&&((#(((##%%#(((((((%/................/%/////////
# .......................,..,,#%*///////(&,.,..........,.##(#(((((((((%##(/((((((((((((((((((##.................,,&///////
# ............................,,(%///%&,...............,../&(&%%((((((((%(((((((((((((((((((((#,..................,,&////*
# ............................,.,.,/,,.,....,............,,,&((((((((((((#((((((((((((((((((((&,.....................,@//*

__author__ = "AIShield"
__copyright__ = "Copyright @2023 Bosch Global Software Technologies Private Limited."
__credits__ = "AIShield"
__license__ = "Apache-2.0"
__version__ = "1.0"
__maintainer__ = "AIShield"
__email__ = "AIShield.Contact@bosch.com"
__status__ = "Beta"

# import libraries
import os
import h5py
import base64
import inspect
from picklescan import scanner
from tensorflow import keras
from keras.layers import Layer, Lambda
import torch
import torch.nn as nn
from safetensors import safe_open
from transformers import (
    AutoModel,
    AutoModelForSequenceClassification,
    AutoModelForTokenClassification,
    AutoModelForQuestionAnswering,
    AutoModelForCausalLM,
    AutoModelForMaskedLM,
    AutoModelForSeq2SeqLM,
    AutoModelForMultipleChoice,
    AutoModelForImageClassification,
    AutoModelForImageSegmentation,
    AutoModelForVision2Seq,
    AutoModelForAudioClassification,
    AutoModelForDocumentQuestionAnswering
)

# Suppress TensorFlow warnings
import tensorflow as tf

tf.get_logger().setLevel('ERROR')


# Define a dummy NoOpLayer class to handle unknown layers during model loading
class NoOpLayer(Layer):
    def __init__(self, **kwargs):
        super(NoOpLayer, self).__init__(**kwargs)

    def call(self, inputs):
        return inputs


# Function to extract layer names from a .h5 model file
def extract_layers_from_h5(model_file_path):
    """
        Extracts the names of layers from an .h5 model file.

        This function reads an HDF5 model file, extracts the layer names from its model configuration,
        and returns a list of layer names. It is commonly used for inspecting the architecture of a
        saved Keras or TensorFlow model.

        Parameters:
            model_file_path (str): The path to the h5 model file .

        Returns:
            list: A list of layer names extracted from the model configuration.

        Raises:
            ValueError: If the model configuration is not found in the .h5 file.
    """

    layers = []

    try:
        with h5py.File(model_file_path, 'r') as f:
            model_config = f.attrs.get('model_config')
            if model_config is None:
                raise ValueError("Model config not found in the .h5 file.")

            # Decode model_config if it's in bytes
            if isinstance(model_config, bytes):
                model_config = model_config.decode('utf-8')

            # Replace certain strings for Python compatibility
            model_config = model_config.replace("null", "None").replace("false", "False").replace("true", "True")

            model_config = eval(model_config)
            for layer_config in model_config['config']['layers']:
                layers.append(layer_config['class_name'])
    except Exception as e:
        print("Error in Extracting Layers from Model due to {}".format(str(e)))

    return layers


def get_custom_objects(layers):
    """
    Get custom layer objects from a list of layer names.

    This function takes a list of layer names and returns a dictionary of custom layer objects.
    Custom layers are typically defined by the user and may not be included in the default Keras
    layers. The dictionary maps layer names to their corresponding custom layer objects.

    Parameters:
        layers (list): A list of layer names for which custom objects are to be retrieved.

    Returns:
        dict: A dictionary mapping layer names to custom layer objects.
    """

    custom_objects = {}

    try:
        keras_layers = dir(keras.layers)

        for layer in layers:
            if layer not in keras_layers:
                custom_objects[layer] = NoOpLayer
    except Exception as e:
        print("Error in Extracting Custom Objects from Model due to {}".format(str(e)))

    return custom_objects


def load_model(model_file_path: str):
    """
    Description: Load the model from file path

    Parameters:
    - model_file_path(str): Model file full-path


    Returns:
    - model: Loaded model object or None if there's an error
    """
    model = None

    # Extract layers from the model file and populate custom_objects
    layers = extract_layers_from_h5(model_file_path)
    custom_objects = get_custom_objects(layers)

    try:
        model = keras.models.load_model(model_file_path, custom_objects=custom_objects, compile=False)
    except Exception as e:
        print("Error loading model: {}".format(str(e)))

    return model


def contains_pickled_data_direct_scan(h5_file_path: str):
    """
        Directly scan an .h5 file for the presence of pickled data patterns.

        This function checks an HDF5 file for the existence of pickled data patterns, which can indicate
        potential security vulnerabilities. Pickled data is a serialization format in Python that can
        execute arbitrary code when deserialized.

        Parameters:
            h5_file_path (str): The path to the HDF5 file to be scanned for pickled data patterns.

        Returns:
            bool: True if pickled data patterns are detected in the file, indicating a potential
                  security issue; False otherwise.

        Note:
        - The function searches for specific "magic numbers" used in Pickle serialization to identify
          potential pickled data.
        - It also checks for certain "opcodes" that can indicate pickling operations within the file.
        - If pickled data patterns are found, the function prints a message and sets the status to True.
          Otherwise, it returns False.

    """

    status = False
    # Pickle's magic numbers for various protocols
    pickle_magic_numbers = [b'\x80\x03', b'\x80\x04', b'\x80\x05']

    # Pickle opcodes that can indicate pickling operations
    suspicious_opcodes = [b'c', b'g', b'(', b'.', b'0']

    try:
        with open(h5_file_path, 'rb') as f:
            file_content = f.read()
            for magic in pickle_magic_numbers:
                if magic in file_content:
                    # Check for subsequent suspicious opcodes
                    index = file_content.find(magic)
                    subsequent_data = file_content[index:index + 10]  # Check next 10 bytes
                    if any(op in subsequent_data for op in suspicious_opcodes):
                        print(f"Detected pickle pattern after magic number: {magic}")
                        status = True

    except Exception as e:
        print("Scanning any pickled Data scan from the model: {}".format(str(e)))

    return status


def unsafe_check_pkl(model_path: str):
    """
    The unsafe_check_pkl function is designed to analyze models with the .pkl extension for potential
    vulnerabilities. If any vulnerabilities are detected, it will provide information about the vulnerability along
    with a severity rating. The severity rating for .pkl files is considered as Medium because Pickle files have the
    potential to execute arbitrary code and involve serialization, which can introduce security risks.

    Parameters:
        - model_path (str): The path to the model file (.pkl) that you want to scan for vulnerabilities.

    Returns: - tool_output (List[str]): A list of vulnerabilities found during the pickle model scanning process. If
    no vulnerabilities are found, an empty list will be returned.

    """
    tool_output = list()
    if model_path.endswith('.pkl'):
        tool_output.append("pickle_file#Severity:Medium - Detected .pkl file. Pickle can execute arbitrary code.")
        print("Detected .pkl file. -Pickle can execute arbitrary code.")

    return tool_output


def unsafe_check_h5(model_path: str):
    """
        The unsafe_check_h5 function is designed to inspect models with the .h5 extension for potential vulnerabilities.
        If any vulnerabilities are detected, it will provide information about the vulnerability along with a severity
        rating. The severity rating helps users understand the potential risk associated with each vulnerability.
        Vulnerability Severity Ratings:
        - High Severity: Pickle/magic numbers detected.
                Reason: Pickle can execute arbitrary code.
        - High Severity: Lambda layer found.
                Reason: Lambda layers can execute arbitrary code.
        - Medium Severity: Non-Keras layer found.
               Reason: Non-Keras custom layers may contain arbitrary code but require explicit loading.
        - Medium Severity: Unknown layer found.
                Reason: The model contains an unknown custom layer, which requires explicit loading.
        - Low Severity: Unable to load the model.
            Reason: The model cannot be loaded, potentially indicating an issue.
       
        Parameters:
            - model_path (str): The path to the model file (.h5) that you want to scan for vulnerabilities.
 
        Returns:
            - tool_output (List[str]): A list of vulnerabilities found during the pickle model scanning process.
                                      If no vulnerabilities are found, an empty list will be returned.
                                       
        """
    tool_output = list()
    lambda_layer_detected = None
    lambda_suspicious_detected = False
    moderate_lamda_suspicious_detected = False
 
    try:
       
        # Check for suspicious patterns in Lambda layers
        suspicious_patterns = [
            "execute_command", "os.system","ossystem","system", "subprocess.run", "subprocess.Popen",
            "open(", "eval(", "exec(", "import os", "import subprocess", "pickle.load", "pickle.loads", "joblib.load"
        ]
        moderate_suspicious_patterns = ["numpy.load"]
       
        # Try loading the model without custom objects
        model = load_model(model_path)
 
        if model is not None:
            for layer in model.layers:
                if isinstance(layer, Lambda):
                    # lambda_layer_detected = True
                    lambda_function = layer.function
                    base64_encoded_function_string = layer.get_config()["function"][0]
                    decoded_bytes = base64.b64decode(base64_encoded_function_string).decode('utf-8', errors='ignore')
                    if any(pattern in decoded_bytes for pattern in suspicious_patterns):
                        lambda_suspicious_detected = True
                    if any(pattern in decoded_bytes for pattern in moderate_suspicious_patterns):
                        moderate_lamda_suspicious_detected = True
                    break
         
        # Check for pickled data in the .h5 file
        magic_number_detected_status = contains_pickled_data_direct_scan(model_path)
 
        # Check for non-Keras layers
        non_keras_layer_detected = any(not hasattr(keras.layers, type(layer).__name__) for layer in model.layers)
 
        # If magic numbers are detected OR a suspicious Lambda layer is detected OR a non-Keras layer is detected,
        # flag the file
        if magic_number_detected_status:
            tool_output.append("magic_number_detected#Severity:High - Potential security concern detected "
                               "in the model. Pickle can execute arbitrary code")
 
        # elif lambda_layer_detected:
        #     tool_output.append("lambda_suspicious#Severity:High - Potential security concern detected "
        #                        "in the model. Lambda layers can execute arbitrary code")
 
 
        # elif lambda_suspicious_detected and moderate_lamda_suspicious_detected:
        #     tool_output.append("lambda_suspicious#Severity:High - Potential security concern detected "
        #                         "in the model, Lambda layers can execute arbitrary code")
        #     print(
        #         "\nPotential security concern detected in the model.Lambda layers can execute "
        #         "arbitrary code.")
        elif lambda_suspicious_detected:
            tool_output.append("lambda_suspicious#Severity:High - Potential security concern detected "
                                "in the model, Lambda layers can execute arbitrary code")
            print(
                "\nPotential security concern detected in the model.Lambda layers can execute "
                "arbitrary code.")
        elif moderate_lamda_suspicious_detected:
            tool_output.append("lambda_suspicious#Severity:Medium - Potential security concern detected "
                                "in the model, Lambda layers can execute arbitrary code")
            print(
                "\nPotential security concern detected in the model.Lambda layers can execute "
                "arbitrary code.")
        elif non_keras_layer_detected:
            tool_output.append("non_keras_layer#Severity:Medium - Potential security concern detected "
                               "in the model. Non-Keras custom layers can contain arbitrary "
                               "code but require explicit loading.")
            print(
                "\nPotential security concern detected in the model, Non-Keras custom layers can contain arbitrary "
                "code but require explicit loading.")
 
    except ValueError as e:
        if "Unknown layer" in str(e):
            tool_output.append("Unknown layer#Severity:Medium - Potential security concern detected "
                               "in the model. Model contains an unknown custom layer."
                               "Requires explicit loading")
            print(
                "Model contains an unknown custom layer. This can be a potential security risk"
                "Requires explicit loading.")
 
    except Exception as e:
        tool_output.append("Error loading model#Severity:Low - Unable to load the Model {}".format(str(e)))
        print("Error loading model: {}".format(str(e)))
 
    return tool_output


# Function to scan a model and return a JSON report

def unsafe_check_pb(model_path: str):
    """
        The unsafe_check_pb function is designed to examine models with the .pb extension for potential vulnerabilities.
        If any vulnerabilities are detected, it will provide information about the vulnerability along with a severity
        rating.

        Vulnerability Severity Ratings:

        - High Severity: Lambda or embedded layer found.
                Reason: Lambda and Embedding layers in TensorFlow allow for
                        arbitrary operations. If an attacker can modify or introduce a lambda function within
                         a saved model,they can potentially execute any operation when the model is loaded.
        - Medium Severity: Non-standard layer found.
                Reason: Non-standard layers, as defined by the unsafe_check_pb tool, include layers like 'InputLayer,
                        ' 'Dense,' 'Activation,' and others. These custom layers or operations, which are not part of
                        the standard TensorFlow library, can introduce vulnerabilities. However, it's important to note
                        that these custom operations typically require explicit loading, meaning there's an additional
                         step before potential execution.

        Parameters:
            - model_path (str): The path to the model file (.pb) that you want to scan for vulnerabilities.

        Returns:
            - tool_output (List[str]): A list of vulnerabilities found during the .pb model scanning process. If
        no vulnerabilities are found, an empty list will be returned.

        """

    tool_output = list()

    try:
        model = tf.keras.models.load_model(model_path)
    except Exception as e:
        print(f"Error loading model: {e}")
        # tool_output.append("Error loading model#Severity:Low - Unable to load the Model {}".format(str(e)))
        return tool_output

    # List of standard TensorFlow layers
    standard_layers = ['InputLayer', 'Dense', 'Activation', 'Dropout', 'Flatten', 'BatchNormalization',
                       'Conv2D', 'MaxPooling2D', 'AveragePooling2D', 'GlobalAveragePooling2D', 'GlobalMaxPooling2D',
                       'Embedding', 'LSTM', 'GRU', 'SimpleRNN', 'TimeDistributed', 'Bidirectional', 'SeparableConv2D',
                       'DepthwiseConv2D', 'UpSampling2D', 'Reshape', 'Permute', 'RepeatVector', 'SpatialDropout2D',
                       'Conv2DTranspose', 'ConvLSTM2D', 'Masking', 'ZeroPadding2D', 'Cropping2D', 'AlphaDropout',
                       'GaussianNoise', 'GaussianDropout', 'RNN', 'LocallyConnected1D', 'LocallyConnected2D']

    # Define a list of suspicious layers
    suspicious_layers = ['Embedding', 'Lambda']

    try:

        # Check for Lambda layers (High Severity)
        if any([layer.__class__.__name__ in suspicious_layers for layer in model.layers]):
            tool_output.append("Lamda_or_Embedding_layer#Severity:High - Detected Lambda operations in the .pb model."
                               "Reason: Lambda/ Embedding layers in TensorFlow allow for arbitrary operations. If an "
                               "attacker can modify or introduce a lambda function within a saved model after which "
                               "they can potentially execute any operation when the model is loaded.")

        elif any([layer.__class__.__name__ not in standard_layers for layer in model.layers]):
            tool_output.append("Non_Standard_layer#Severity:Medium - Detected non-standard layers in the .pb model."
                               "Reason: Custom layers or operations that are not part of the standard TensorFlow "
                               "library can introduce vulnerabilities. However these custom operations typically "
                               " require explicit loading which means there's an additional step before potential "
                               "execution.")
    except Exception as e:
        print("Failed to Perform unsafe-check-pb due to: {}".format(str(e)))

    return tool_output


def unsafe_check_pytorch_safetensors(model_path: str):
    """
        The unsafe_check_pytorch function is designed to examine pytorch models with the .pt or .pth extension for potential vulnerabilities.
        If any vulnerabilities are detected, it will provide information about the vulnerability along with a severity
        rating.

        Vulnerability Severity Ratings:

        - High Severity: Lambda or embedded layer found.
                Reason: Lambda and Embedding layers allow for
                        arbitrary operations. If an attacker can modify or introduce a lambda function within
                         a saved model,they can potentially execute any operation when the model is loaded.
        - Medium Severity: Non-standard layer found.
                Reason: Non-standard layers, as defined by the unsafe_check_pytorch tool, include layers like 'InputLayer,
                        ' 'Dense,' 'Activation,' and others. These custom layers or operations, which are not part of
                        the standard Pytorch library, can introduce vulnerabilities. However, it's important to note
                        that these custom operations typically require explicit loading, meaning there's an additional
                         step before potential execution.

        Parameters:
            - model_path (str): The path to the model file (.pt or .pth) that you want to scan for vulnerabilities.

        Returns:
            - tool_output (List[str]): A list of vulnerabilities found during the .pt or .pth model scanning process. If
        no vulnerabilities are found, an empty list will be returned.

        """

    tool_output = list()
    model_file_path = model_path  # Assigning this so that if none of the transformer loading methods work, load the model using torch.load()

    if model_file_path.endswith(".safetensors"):
        use_safetensors_file = True
    else:
        use_safetensors_file = False

    if os.path.isfile(model_path):
        model_path = os.path.dirname(model_path)

    # List of suspicious patterns to detect
    suspicious_patterns = [
        "execute_command", "os.system", "subprocess.run", "subprocess.Popen",
        "open(", "eval(", "exec(", "import os", "import subprocess", "pickle.load", "pickle.loads", "joblib.load",
        "CustomLayer"
    ]
    moderate_suspicious_patterns = ["numpy.load"]

    model_classes = [
        AutoModel,
        AutoModelForSequenceClassification,
        AutoModelForTokenClassification,
        AutoModelForQuestionAnswering,
        AutoModelForCausalLM,
        AutoModelForMaskedLM,
        AutoModelForSeq2SeqLM,
        AutoModelForMultipleChoice,
        AutoModelForImageClassification,
        AutoModelForImageSegmentation,
        AutoModelForVision2Seq,
        AutoModelForAudioClassification,
        AutoModelForDocumentQuestionAnswering,
    ]

    model = None
    for model_class in model_classes:
        try:
            model = model_class.from_pretrained(model_path, trust_remote_code=True,
                                                use_safetensors=use_safetensors_file)
            break  #If model is getting loaded through any of the method, break out of loop with loaded model
        except Exception as e:
            if model_class == model_classes[-1]:
                print(f"Failed to load model using transformer functions, trying it with torch.load(): {e}")
            else:
                continue

    # If unable to load the model through transformer function, try to load it using torch.load()
    if model is None:
        try:
            if model_file_path.endswith(('.pt', '.pth')):
                model = torch.load(model_file_path)
        except Exception as e:
            print(f"Failed to load model: {e}")

    if model is None:
        # tool_output.append("Error loading model#Severity:Low - Unable to load the Model {}")
        return tool_output

    # List of Standard Pytorch Layers
    standard_layers = [
        'Linear', 'Conv2d', 'BatchNorm2d', 'ReLU', 'MaxPool2d', 'AvgPool2d',
        'AdaptiveAvgPool2d', 'AdaptiveMaxPool2d', 'Dropout', 'Flatten', 'RNN',
        'LSTM', 'GRU', 'Embedding', 'LayerNorm'  # Included LayerNorm
    ]

    # Define a list of suspicious layers
    suspicious_layers = ['Lambda', "CustomLayer", "SuspiciousModel"]
    lambda_suspicious = False
    not_standard_detected = False
    high_suspicious_pattern = False
    moderate_suspicious_pattern = False

    try:
        for name, layer in model.named_modules():
            # Check if the layer is not a standard PyTorch module
            if not isinstance(layer, nn.Module):
                not_standard_detected = True
            # print(name)
            # Check for suspicious layer names
            if any(susp_layer in name for susp_layer in suspicious_layers):
                lambda_suspicious = True

            # Check the source code of the forward method
            if hasattr(layer, 'forward'):
                try:
                    function_code = inspect.getsource(layer.forward)
                    #print("function code to inspect",function_code)
                    if any(pattern in function_code for pattern in suspicious_patterns):
                        high_suspicious_pattern = True

                    elif any(pattern in function_code for pattern in moderate_suspicious_patterns):
                        moderate_suspicious_pattern = True
                except Exception as e:
                    print(f"Error inspecting {name}: {str(e)}")

        if lambda_suspicious or high_suspicious_pattern:
            tool_output.append(
                "Lambda_or_Custom_layer#Severity:High - Detected Lambda or Custom operations or suspicious pattern in "
                "layers of pytorch model."
                "Reason: Lambda or custom layers in PyTorch models allow for arbitrary operations. If an attacker can "
                "modify or introduce a lambda function within the model after which they can potentially execute any "
                "operation when the model is loaded or may be high suspicious pattern is found that can execute "
                "arbitrary code in model"
            )
        elif not_standard_detected or moderate_suspicious_pattern:
            tool_output.append(
                "Non_Standard_layer#Severity:Medium - Detected non-standard layers in the pytorch model or found any "
                "moderate suspicious pattern in layers"
                "Reason: Custom layers or operations that are not part of the standard PyTorch library can introduce "
                "vulnerabilities. However these custom operations typically require explicit loading which means "
                "there's"
                "an additional step before potential execution."
            )

    except Exception as e:
        print("Failed to Perform unsafe-check-pytorch due to: {}".format(str(e)))

    return tool_output

def unsafe_check_safetensors(model_path: str):
    """
    The unsafe_check_safetensors function is designed to examine safetensors models converted from base pytorch
    models with the .safetensors extension for potential vulnerabilities. If any vulnerabilities are detected,
    it will provide information about the vulnerability along with a severity rating.

        Vulnerability Severity Ratings:

        - High Severity: Lambda or embedded layer found.
                Reason: Lambda and Embedding layers allow for
                        arbitrary operations. If an attacker can modify or introduce a lambda function within
                         a saved model,they can potentially execute any operation when the model is loaded.
        - Medium Severity: Non-standard layer found.
                Reason: Non-standard layers, as defined by the unsafe_check_safetensors tool, include layers like 'InputLayer,
                        ' 'Dense,' 'Activation,' and others. These custom layers or operations, which are not part of
                        the standard Pytorch library, hence can cause vulnerabilities when it is converted to safetensors
                        format can introduce vulnerabilities. However, it's important to note
                        that these custom operations typically require explicit loading, meaning there's an additional
                        step before potential execution.

        Parameters:
            - model_path (str): The path to the model file (.safetensors) that you want to scan for vulnerabilities.

        Returns:
            - tool_output (List[str]): A list of vulnerabilities found during the .safetensors model scanning process.
            If no vulnerabilities are found, an empty list will be returned.

        """
    
    tool_output = list()
    model_layers = list() # Create a list to append all the layers of the safetensors model
    
    # List of suspicious patterns to detect
    suspicious_patterns = [
        "execute_command", "os.system", "subprocess.run", "subprocess.Popen",
        "open", "eval", "exec", "import os", "import subprocess", "pickle.load", "pickle.loads", "joblib.load"
    ]
    moderate_suspicious_patterns = ["numpy.load"]
    
    try:
        # Load the model from the SafeTensors file
        with safe_open(model_path, framework="pt") as f: # Since the base model is a pytorch model, specifying the framework as pt
            layer_names = f.keys()
            model_layers.extend(layer_names)
    except Exception as e:
        print(f"Error loading model: {e}")
        tool_output.append("Error loading model#Severity:Low - Unable to load the Model {}".format(str(e)))
        return tool_output
    
    # List of standard Pytorch base safetensor layers
    standard_layers = [
        'linear', 'conv2d', 'batchnorm2d', 'relu', 'maxpool2d', 'avgpool2d',
        'adaptiveavgpool2d', 'adaptivemaxpool2d', 'dropout', 'flatten', 'rnn',
        'lstm', 'gru', 'embedding', 'layernorm',
        'InstanceNorm2d', 'GroupNorm', 'LocalResponseNorm',
        'GlobalAvgPool2d', 'GlobalMaxPool2d',
        'sigmoid', 'tanh', 'softmax',
        'transformer', 'attention', 'upsample',
        'identity', 'concat', 'reshape',
        'gaussiannoise', 'alphadropout', 'threshold'
    ]
    # Define a list of suspicious layers
    suspicious_layers = ['Lambda', "CustomLayer"]
    lamda_suspicious_layer = False
    non_standard_layer = False
    high_suspicious_pattern = False
    moderate_suspicious_pattern = False
    try:
        for layer in model_layers:
            layer_name = layer.split('.')[-2]  # Get the actual layer name

            if any(layer_name == suspicious_layer for suspicious_layer in suspicious_layers):
                lamda_suspicious_layer=True
                break
            elif any(layer_name not in standard_layers for layer_name in layer.split('.')):
                non_standard_layer = True
                
        for pattern in suspicious_patterns:
            if any(pattern in layer for layer in model_layers):
                high_suspicious_pattern=True
                break

        for pattern in moderate_suspicious_patterns:
            if any(pattern in layer for layer in model_layers):
                moderate_suspicious_pattern=True
        
        if lamda_suspicious_layer or high_suspicious_pattern:
            tool_output.append("Lamda_or_Embedding_layer#Severity:High - Detected Lambda operations in the "
                               ".safetensors model or Detected suspicious pattern  in the .safetensors model. "
                               "Reason: Lambda/ Custom layers in SafeTensors models allow for arbitrary "
                               "operations  or if suspicious pattern found that may indicate an attempt to execute arbitrary code when the "
                               " model is loaded. If an "
                               "attacker can modify or introduce a lambda function within the model, they can "
                               "potentially execute any operation when the model is loaded."
                               )
        elif moderate_suspicious_pattern or non_standard_layer:
            tool_output.append("Non_Standard_layer#Severity:Medium - Detected non-standard layers in the "
                               ".safetensors model or detected moderate suspicious pattern  in the "
                               ".safetensors model. ."
                               "Reason: Custom layers or operations that are not part of the standard SafeTensors Model "
                               "can introduce vulnerabilities. However, these custom operations typically "
                               " require explicit loading, which means there's an additional step before potential "
                               "execution. or any suspicious pattern may indicate to attempt to load the arbitrary data when the model is loaded")
                
    except Exception as e:
        print("Failed to Perform unsafe-check-safetensors due to: {}".format(str(e)))
        
    return tool_output
    
    
def scan_pickle_file(path: str):
    """
    This function takes pickle file path as input to scan for static vulnerability in pickle file
    
    severity mapping as below. only 3 format the safety is given by scanner
    Low = "innocuous"
    Medium = "suspicious"
    High = "dangerous"

    Parameters
    ----------
    path : str
        DESCRIPTION. full path of file

    Returns
    -------
    None.

    """
    results = []
    pickle_scan_result = scanner.scan_file_path(path=path)
    for pickle_vulnerability in pickle_scan_result.globals:
        module = pickle_vulnerability.module
        name = pickle_vulnerability.name
        safety = pickle_vulnerability.safety.value
        severity = "High" if safety == 'dangerous' else "Medium" if safety == 'suspicious' else "Low"
        results.append({'module': module, "name": name, "severity": severity})

    # Other variable of scanner class
    # pickle_scan_result.scanned_files
    # pickle_scan_result.issues_count
    # pickle_scan_result.infected_files
    # pickle_scan_result.scan_err

    return results